DIVE INTO DEEP LEARNING

# 2.é¢„å¤‡çŸ¥è¯†

## 2.1 æ•°æ®æ“ä½œ

**åˆ›å»ºæ•°æ®**

```python
torch.arange(12)         #ç±»ä¼¼range
torch.zeros((2,3,4))
torch.ones((2,3,4))
torch.randn((3,4))       #éšæœºå‡½æ•°
torch.tensor([[2,1,4,3],[1,2,3,4],[4,3,2,1]])  #åˆ—è¡¨è½¬å¼ é‡
```

**æ•°æ®å¤„ç†**

```python
x+y
x-y
x*y
x/y
x**y               #yä¸­å…ƒç´ æ˜¯xçš„å¹‚æ¬¡
torch.exp(x)       #å¯¹xçš„å…ƒç´ é€ä¸ªexp
X = x.reshape(3,4) #æ”¹å˜å½¢çŠ¶ï¼Œå¤§å°ä¸å˜ã€‚å¯ä»¥ç”¨-1ä»£è¡¨æ‡’å¾—ç®— å¦‚x.reshape(-1,4)
```

**æ•°æ®ä¿¡æ¯**

```python
x.shape            #å½¢çŠ¶
x.numel()          #å¤§å°
```

## 2.2 æ•°æ®é¢„å¤„ç†

pandas

å¯¹ç¼ºå¤±çš„ï¼Œè¡¨ç°ä¸ºNaNå€¼ï¼Œå¯ä»¥ç”¨pd.get_dummiesæ–¹æ³•

```python
inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]
inputs = inputs.fillna(inputs.mean()) # å¯¹æ•°å€¼åŸŸçš„å…ƒç´ ä¸ºNaNçš„ï¼Œå¡«å……ä¸ºå…¶ä»–æ•°å€¼åŸŸçš„å‡å€¼
print(inputs)
```

|      | NumRooms | Alley |
| :--: | :------: | :---: |
|  0   |   3.0    | Pave  |
|  1   |   2.0    |  NaN  |
|  2   |   4.0    |  NaN  |
|  3   |   3.0    |  NaN  |

```python
inputs = pd.get_dummies(inputs, dummy_na=True)
inputs
```

|      | NumRooms | Alley_Pave | Alley_nan |
| :--: | :------: | :--------: | :-------: |
|  0   |   3.0    |     1      |     0     |
|  1   |   2.0    |     0      |     1     |
|  2   |   4.0    |     0      |     1     |
|  3   |   3.0    |     0      |     1     |

## 2.3 çº¿æ€§ä»£æ•°

**çŸ©é˜µè½¬ç½®**ï¼š å»¶å¯¹è§’çº¿åè½¬

**å¯¹ç§°çŸ©é˜µ**ï¼š B ==B.T

**çŸ©é˜µåˆ›å»º**

```python
x = torch.arange(24).reshape(2,3,4)
```

**çŸ©é˜µå¯¹åº”å…ƒç´ ç›¸ä¹˜ï¼ˆHadamard productï¼‰**

```python
A * B
```

**é™ç»´**    sumå‡½æ•°ä¸­çš„axisï¼Œ0ä¸ºæŒ‰è¡Œæ±‚å’Œï¼ˆæ¯ä¸€åˆ—çš„æ±‚å’Œä½œä¸ºä¸€ä¸ªå€¼ï¼‰ï¼Œ1ä¸ºæŒ‰åˆ—æ±‚å’Œï¼Œä»¥æ­¤ç±»æ¨

â€‹			keepdims=Trueï¼Œä½¿è¢«æ±‚å’Œçš„ç»´åº¦çš„å¤§å°é™ä¸º1ã€‚å³æŒ‰åŸæœ¬ç»´åº¦è¾“å‡ºã€‚ï¼ˆé»˜è®¤çš„è¯ï¼Œæ— è®ºaxisä¸ºå¤šå°‘ï¼Œéƒ½æ˜¯æŒ‰ç…§è¡Œåˆ—å¼€å§‹è¾“å‡ºçš„ï¼‰ ç”¨äºå¹¿æ’­æœºåˆ¶ã€‚

**ç‚¹ç§¯**

```python
torch.dot(x,y)  #ç­‰ä»·äºä¸‹å¼
torch.sum(x*y)
```

**å‘é‡ç§¯**

```python
torch.mm(A,B)
```

**èŒƒæ•°**

```python
torch.norm(u)          #é•¿åº¦ï¼Œå„å…ƒç´ å¹³æ–¹å’Œçš„å¹³æ–¹æ ¹
torch.abs(u).sum()     #L1èŒƒæ•°ï¼Œå„å…ƒç´ ç»å¯¹å€¼çš„å’Œ
```

## 2.5 è‡ªåŠ¨å¾®åˆ†

è‡ªå˜é‡xï¼Œå› å˜é‡y

**å°†xçš„requires_gradè®¾ç½®ä¸ºTrueï¼Œè‡ªåŠ¨è®°å½•xçš„æ¢¯åº¦**

```python
x = torch.arange(4.0)
y = 2 * torch.dot(x,x)
x.requires_grad_(True) # ç­‰ä»·äº x=torch.arange(4.0,requires_grad=True)
```

**è°ƒç”¨åå‘ä¼ æ’­å‡½æ•°è®¡ç®—yå…³äºxçš„æ¢¯åº¦**

```python
y.backward()
x.grad
```

ç»“æœï¼štensor([ 0.,  4.,  8., 12.])

# 3.çº¿æ€§ç¥ç»ç½‘ç»œ

## 3.1 çº¿æ€§å›å½’

çº¿æ€§æ¨¡å‹ï¼šå•å±‚ç¥ç»ç½‘ç»œï¼›

å¯¹nç»´è¾“å…¥åŠ æƒï¼ŒåŠ åå·®ï¼›

ç”¨å¹³æ–¹æŸå¤±æ¥è¡¡é‡é¢„æµ‹å’ŒçœŸå®å€¼çš„å·®å¼‚ï¼›

ä¸¤ä¸ªè¶…å‚æ•°hyperparametersï¼šå­¦ä¹ é€Ÿç‡å’Œæ‰¹é‡å¤§å°ï¼ˆæ•´ä¸ªåœ¨è®­ç»ƒé›†ä¸Šç®—æ¢¯åº¦å¤ªè´µäº†ï¼Œéœ€è¦æ¯æ¬¡é‡‡æ ·bä¸ªæ ·æœ¬æ¥è¿‘ä¼¼ï¼‰ï¼›

æ¨¡å‹å‚æ•°ï¼šæ¨¡å‹é‡Œçš„å‚æ•°ï¼Œå¦‚wï¼Œbï¼›

æ·±åº¦å­¦ä¹ çš„é»˜è®¤æ±‚è§£ç®—æ³•ï¼šå°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™

## 3.2 çº¿æ€§å›å½’ä»é›¶å¼€å§‹å®ç°

### 3.2.1 ç”Ÿæˆæ ·æœ¬é›†

```python
def synthetic_data(w,b,num_examples):
    x = torch.normal(0,1,(num_examples,len(w)))
    y = torch.matmul(x,w)+b
    y += torch.normal(0,0.01,y.shape)
    return x,y.reshape((-1,1))

true_w = torch.tensor([2,-3.4])
true_b = 4.2
features,labels = synthetic_data(true_w,true_b,1000)
```

torch.matmul   # Matrix product of two tensors

torch.mm          # Supports strided and sparse 2-D tensors as inputs, autograd with respect to strided inputs. not support broadcasting

**æŸ¥çœ‹æ ·æœ¬é›†ï¼š**

```python
d2l.set_figsize()
d2l.plt.scatter(features[:,1].detach().numpy(), 
               labels.detach().numpy(), 1);
```

detach() # è¿”å›ä¸€ä¸ªæ–°çš„tensorï¼Œä»å½“å‰è®¡ç®—å›¾ä¸­åˆ†ç¦»ä¸‹æ¥çš„ï¼Œä½†æ˜¯ä»æŒ‡å‘åŸå˜é‡çš„å­˜æ”¾ä½ç½®,ä¸åŒä¹‹å¤„åªæ˜¯requires_gradä¸ºfalseï¼Œå¾—åˆ°çš„è¿™ä¸ªtensoræ°¸è¿œä¸éœ€è¦è®¡ç®—å…¶æ¢¯åº¦ï¼Œä¸å…·æœ‰gradã€‚

### 3.2.2 ç”Ÿæˆå°æ‰¹é‡æ•°æ®

```python
def data_iter(batch_size,features,labels):
    num_examples = len(labels)
    indices = list(range(num_examples))
    random.shuffle(indices)
    for i in range(0,num_examples,batch_size):
        batch_indices = torch.tensor(indices[i,min(i+batch_size,num_examples)])
        yield features[batch_indices],labels[batch_indices]
```

*torché‡Œé¢çš„ tensorç«Ÿç„¶æ”¯æŒå¼ é‡åµŒå¥—0.0 æ— æ•Œäº†ğŸ‘€* 

> *æ¯”å¦‚ï¼š* 
>
> *a = torch.tensor([1,2,3,4,5,6,7])*
> *b = torch.tensor([2,3,4])*
> *é‚£ä¹ˆ a[b]çš„å€¼ä¸º  tensor([3, 4, 5])*

### 3.2.3 å®šä¹‰åˆå§‹åŒ–æ¨¡å‹å‚æ•°å’Œæ¨¡å‹

```python
w = torch.normal(0,0.01,size=(2,1),requires_grad=True)
b = torch.zeros(1,requires_grad=True)

def linreg(X,w,b):
    return torch.matmul(X,w)+b
```

### 3.2.4 å®šä¹‰æŸå¤±å‡½æ•°

```python
def squared_loss(y_hat,y):
    return(y_hat - y.reshape(y_hat.shape))**2/2
```

### 3.2.5 å®šä¹‰ä¼˜åŒ–ç®—æ³•

```python
def sgd(params,lr,batch_size):
    with torch.no_grad():
        for param in params:
            param -= lr* param.grad / batch_size
            param.grad.zero_()
```

### 3.2.6 è®­ç»ƒ

```python
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss

for epoch in range(num_epochs):
    for X,y in data_iter(batch_size,features,labels):
        l = loss(net(X,w,b),y)
        l.sum().backward()
        sgd([w,b],lr,batch_size)
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
```

## 3.3 çº¿æ€§å›å½’çš„ç®€æ´å®ç°

åˆå§‹æ“ä½œ

```python
import torch
from torch.utils import data
from d2l import torch as d2l repe
import numpy as np

true_w = torch.tensor([2,-3.4])
true_b = 4.2
features,labels = d2l.synthetic_data(true_w,true_b,1000)
```

### 3.3.1 è¿”å›ç”Ÿæˆå¹¶è¿”å›å°æ‰¹é‡æ•°æ®

```python
def load_array(data_arrays,batch_size,is_train=True):
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset,batch_size,shuffle=is_train)

batch_size = 10
data_iter = load_array((features,labels),batch_size)

```

### 3.3.2 å®šä¹‰å±‚

```python
from torch import nn 

net = nn.Sequential(nn.Linear(2,1))
```

nn ï¼šneural network ç¥ç»ç½‘ç»œï¼›

æŒ‡å®šè¾“å…¥ç»´åº¦ä¸º2ï¼Œè¾“å‡ºç»´åº¦ä¸º1ï¼›

å¯ä»¥ç›´æ¥ç”¨ net = nn.Linear(2,1)ï¼Œå› ä¸ºçº¿æ€§å›å½’æ˜¯å•å±‚çš„ï¼›

### 3.3.3 åˆå§‹åŒ–å‚æ•°æ¨¡å‹

```python
net[0].weight.data.normal_(0,0.01)
net[0].bias.data.fill_(0)
```

**å…¶å®ä¹Ÿå¯ä»¥ä¸ç”¨è®¾ç½®**

net[0]ï¼š è®¿é—®netçš„ç¬¬0å±‚ï¼›

è®¿é—®æƒé‡çš„dataï¼Œå°†å…¶è®¾ä¸ºæ­£å¤ªåˆ†å¸ƒï¼›

å°†biasè®¾ä¸º0ï¼›

### 3.3.4 è®¾ç½®losså’ŒSGD

SGDï¼ˆstochastic gradient descentï¼‰éšæœºæ¢¯åº¦ä¸‹é™

```python
loss = nn.MSELoss()
trainer = torch.optim.SGD(net.parameters(),lr=0.03)
```

net.parameters()ä¸º çº¿æ€§å›å½’å±‚ä¸­çš„æ‰€æœ‰å‚æ•°ï¼ˆwå’Œbï¼‰

### 3.3.5 è®­ç»ƒ

```python
num_epochs = 3
for epoch in range(num_epochs):
    for X ,y in data_iter:
        l = loss(net(X),y)
        trainer.zero_grad()
        l.backward()
        trainer.step()
    l = loss(net(features),labels)
    print(f'epoch {epoch + 1}, loss {l:f}')
```

## 3.4 softmax å›å½’

å›å½’ vs åˆ†ç±»

- å›å½’ä¼°è®¡ä¸€ä¸ªè¿ç»­å€¼
- åˆ†ç±»é¢„æµ‹ä¸€ä¸ªç¦»æ•£ç±»åˆ«

å¯¹softmax è¾“å…¥Xï¼Œè¾“å‡ºy_hatï¼ˆä¸€ç»´å¼ é‡ï¼Œsumä¸º1ï¼‰ï¼Œæ“ä½œä¸ºsoftmaxï¼Œå…¶ä¸­oæ˜¯æœªè§„èŒƒåŒ–çš„é¢„æµ‹

![image-20211230193114538](C:\Users\DAI FENGYUAN\AppData\Roaming\Typora\typora-user-images\image-20211230193114538.png)

å°†y_hatå’ŒçœŸå®å€¼yçš„åŒºåˆ«ä½œä¸ºæŸå¤±ã€‚ä¸‹å¼è¯´æ˜å›å½’é—®é¢˜åªå…³å¿ƒå¯¹çœŸå®å€¼çš„é¢„æµ‹æ¦‚ç‡ã€‚

$$
l(y,y') = - \sum_{j=1}^{q}{y_jlogy_j})
$$
å…¶æ¢¯åº¦æ°å¥½æ˜¯çœŸå®æ¦‚ç‡å’Œé¢„æµ‹æ¦‚ç‡çš„å·®å€¼ ***ï¼ˆé—®é¢˜ï¼šè¿™æ€ä¹ˆç®—çš„ï¼Ÿï¼‰***

![image-20211230194135919](C:\Users\DAI FENGYUAN\AppData\Roaming\Typora\typora-user-images\image-20211230194135919.png)

## 3.5 å›¾åƒåˆ†ç±»æ•°æ®é›†

å¼•å…¥çš„ä¸€äº›åº“

```python
%matplotlib inline
import torch
import torchvision
from torch.utils import data
from torchvision import transforms
from d2l import torch as d2l

d2l.use_svg_display()
```

ä¸‹è½½æ•°æ®é›†

```python
# é€šè¿‡ToTensorå®ä¾‹å°†å›¾åƒæ•°æ®ä»PILç±»å‹å˜æ¢æˆ32ä½æµ®ç‚¹æ•°æ ¼å¼ï¼Œ
# å¹¶é™¤ä»¥255ä½¿å¾—æ‰€æœ‰åƒç´ çš„æ•°å€¼å‡åœ¨0åˆ°1ä¹‹é—´
trans = transforms.ToTensor()
mnist_train = torchvision.datasets.FashionMNIST(
    root="../data", train=True, transform=trans, download=True)
mnist_test = torchvision.datasets.FashionMNIST(
    root="../data", train=False, transform=trans, download=True)
```

å¯¹æ•°æ®é›†çš„è¯´æ˜ï¼š

- mnist_trainæ˜¯è®­ç»ƒé›†ï¼Œmnist_testæ˜¯æµ‹è¯•é›†

- ```python
  len(mnist_train), len(mnist_test)
  ```

  ```
  (60000, 10000)
  ```

- mnist_train[i]æ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯shapeä¸º([1,28,28])çš„é€šé“æ•°ä¸º1ï¼Œé«˜åº¦å’Œå®½åº¦å‡ä¸º28åƒç´ çš„å›¾åƒï¼Œç¬¬äºŒä¸ªshapeæ˜¯ä¸€ä¸ªintï¼ŒæŒ‡æ˜è¯¥å›¾åƒçš„ç±»å‹

å¯è§†åŒ–æ•°æ®    ***å…¶å®ä¸æ˜¯å¾ˆç†è§£***

```python
def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save
    """ç»˜åˆ¶å›¾åƒåˆ—è¡¨"""
    figsize = (num_cols * scale, num_rows * scale)
    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)
    axes = axes.flatten()
    for i, (ax, img) in enumerate(zip(axes, imgs)):
        if torch.is_tensor(img):
            # å›¾ç‰‡å¼ é‡
            ax.imshow(img.numpy())
        else:
            # PILå›¾ç‰‡
            ax.imshow(img)
        ax.axes.get_xaxis().set_visible(False)
        ax.axes.get_yaxis().set_visible(False)
        if titles:
            ax.set_title(titles[i])
    return axes

def get_fashion_mnist_labels(labels):  #@save
    """è¿”å›Fashion-MNISTæ•°æ®é›†çš„æ–‡æœ¬æ ‡ç­¾"""
    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
    return [text_labels[int(i)] for i in labels]

X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))
show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y));
```

## 3.6 softmaxå›å½’çš„ä»é›¶å¼€å§‹å®ç°

è¿™èŠ‚çœŸçš„æœ‰ç‚¹éš¾å•Š

### 3.6.1 å¯¼å…¥æ•°æ®ï¼Œå¹¶åˆå§‹åŒ–è¾“å…¥å‚æ•°

```python
import torch
from IPython import display
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

```python
num_inputs = 784
num_outputs = 10

W = torch.normal(0,0.01,size = (num_inputs,num_outputs),requires_grad = True)
b = torch.zeros(num_outputs,requires_grad = True)
```

### 3.6.2 å®šä¹‰softmaxä»¥åŠæ¨¡å‹

```python
def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1,keepdim=True)
    return X_exp / partition

def net(X):
    return softmax(torch.matmul(X.reshape(-1,W.shape[0],W)+b))
```

### 3.6.3 å®šä¹‰losså‡½æ•°

æ³¨ï¼špytorchçš„torchåµŒå¥—è¿˜å¯ä»¥è¿™ä¹ˆç”¨

```python
y = torch.tensor([0, 2])
y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])
y_hat[[0,1],y]
```

å…¶ç»“æœä¸ºï¼štensor([0.1000, 0.5000])  å§æ§½ç¦»è°±`(*>ï¹<*)â€²

å®ç°äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œå…¬å¼å¦‚ä¸‹ï¼ˆy'å³ä¸ºy_hatï¼‰
$$
l(y,y') = - \sum_{j=1}^{q}{y_jlogy_j})
$$
ä»£ç å¦‚ä¸‹

```python
def cross_entropy(y_hat, y):
    return -torch.log(y_hat[range(len(y_hat)),y])
```

### 3.6.4 åˆ†ç±»ç²¾åº¦

è¿™éƒ¨åˆ†æœ‰ç‚¹å¤æ‚ æˆ‘æœ‰ç‚¹æ‡µã€‚ã€‚ã€‚

```python
def accuracy(y_hat,y):
    '''è®¡ç®—é¢„æµ‹æ­£ç¡®çš„æ•°é‡'''
    if len(y_hat.shape)>1 and y_hat.shape[1]>1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    return float(cmp.type(y.dtype).sum())
```

```python
def evaluate_accuracy(net, data_iter):  #@save
    """è®¡ç®—åœ¨æŒ‡å®šæ•°æ®é›†ä¸Šæ¨¡å‹çš„ç²¾åº¦"""
    if isinstance(net, torch.nn.Module):
        net.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼,ä¸ç”¨è®¡ç®—æ¢¯åº¦ï¼Œj
    metric = Accumulator(2)  # æ­£ç¡®é¢„æµ‹æ•°ã€é¢„æµ‹æ€»æ•°
    with torch.no_grad():
        for X, y in data_iter:
            metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]
```

å…¶ä¸­accumulatoræ˜¯ä¸€ä¸ªç´¯åŠ å™¨ï¼Œå…¶å®šä¹‰å¦‚ä¸‹

```python
class Accumulator:  #@save
    """åœ¨nä¸ªå˜é‡ä¸Šç´¯åŠ """
    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
```

### 3.6.5 è®­ç»ƒ

æ¯ä¸ªepochçš„è®­ç»ƒï¼šæ­¤è®­ç»ƒåŒ…å«äº†åé¢softmoxç®€æ´å®ç°

```python
def train_epoch_ch3(net, train_iter, loss, updater):  #@save
    """è®­ç»ƒæ¨¡å‹ä¸€ä¸ªè¿­ä»£å‘¨æœŸï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰"""
    # å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    if isinstance(net, torch.nn.Module):
        net.train()
    # è®­ç»ƒæŸå¤±æ€»å’Œã€è®­ç»ƒå‡†ç¡®åº¦æ€»å’Œã€æ ·æœ¬æ•°
    metric = Accumulator(3)
    for X, y in train_iter:
        # è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            # ä½¿ç”¨PyTorchå†…ç½®çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            updater.zero_grad()
            l.sum().backward()
            updater.step()
        else:
            # ä½¿ç”¨å®šåˆ¶çš„ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
            l.sum().backward()
            updater(X.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # è¿”å›è®­ç»ƒæŸå¤±å’Œè®­ç»ƒç²¾åº¦
    return metric[0] / metric[2], metric[1] / metric[2]
```

åé¢ç”¨matplotlibç”»äº†ä¸€ä¸ªå›¾ï¼Œç”¨äºå¯è§†åŒ–ï¼Œå°±ä¸è´´ä»£ç äº†0.0

è®­ç»ƒæ¨¡å‹

```python
def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save
    """è®­ç»ƒæ¨¡å‹ï¼ˆå®šä¹‰è§ç¬¬3ç« ï¼‰"""
    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                        legend=['train loss', 'train acc', 'test acc'])
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        animator.add(epoch + 1, train_metrics + (test_acc,))
    train_loss, train_acc = train_metrics
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc
```

æœ€åå®šä¹‰updaterï¼ˆå°±æ˜¯æ¯æ¬¡æ²¿æ¢¯åº¦æ›´æ–°ä¸€ä¸‹parametersçš„å€¼ï¼Œï¼‰

```python
lr = 0.1

def updater(batch_size):
    return d2l.sgd([W, b], lr, batch_size)
```

ç„¶åå¼€å§‹è®­ç»ƒ

```python
num_epochs = 15
train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)
```

## 3.7 softmaxå›å½’çš„ç®€æ´å®ç°

### 3.7.1 å¯¼å…¥ä¸€äº›åº“

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size)
```

### 3.7.2 åˆå§‹åŒ–æ¨¡å‹å‚æ•°

åœ¨çº¿æ€§å±‚å‰å®šä¹‰äº†å±•å¹³å±‚ï¼ˆflattenï¼‰ï¼Œæ¥è°ƒæ•´ç½‘ç»œè¾“å…¥çš„å½¢çŠ¶ï¼ˆç±»ä¼¼ä»é›¶å®ç°çš„reshapeï¼‰

nn.init.normal_ ï¼šåœ¨å¯¹çº¿æ€§å±‚ä¸­çš„æ¯ä¸€ä¸ªæƒé‡éƒ½è®¾ç½®æˆå‡å€¼ä¸º0ï¼Œé»˜è®¤ä¸º0ï¼Œæ–¹å·®ä¸º0.01çš„éšæœºå€¼ï¼ˆå¥½åƒåé¢æœ‰è®²ç©¶çš„ï¼‰

net.apply ï¼šå¯¹æ¯ä¸€ä¸ªlayeréƒ½callä¸€æ¬¡

```python
net = nn.Sequential(nn.Flatten(),nn.Linear(784,10))

def init_weight(m):   
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight,std=0.01)  
net.apply(init_weight)
```

### 3.7.3 losså‡½æ•°å’Œè®­ç»ƒæ–¹æ³•

nn.CrossEntropyLoss() ï¼šäº¤å‰ç†µæŸå¤±å‡½æ•°

```python
loss = nn.CrossEntropyLoss()
trainer = torch.optim.SGD(net.parameters(),lr=0.01)
```

### 3.7.4 è®­ç»ƒ

æ²¡å•¥å¥½è¯´çš„ï¼Œç”¨ä¸Šä¸€èŠ‚çš„å‡½æ•°å°±å®Œäº‹äº†

```python
num_epochs = 10
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

# 4 å¤šå±‚æ„ŸçŸ¥æœº MLP(Multilayer Perceptron)

å•å±‚æ„ŸçŸ¥æœºä¸èƒ½æ‹ŸåˆXORï¼ˆå¼‚æˆ–ï¼‰å‡½æ•°ï¼Œåªèƒ½åˆ†å‰²çº¿æ€§é¢

- è§£å†³æ–¹æ³•ï¼šç”¨å¤šå±‚æ„ŸçŸ¥æœº

å¤šå±‚æ„ŸçŸ¥æœºä¸åŠ æ¿€æ´»å‡½æ•°çš„è¯ï¼Œè¿˜æ˜¯çº¿æ€§å›å½’

## 4.1 å¤šå±‚æ„ŸçŸ¥æœº

### 4.1.1 æ¿€æ´»å‡½æ•°

Sigmoidå‡½æ•° 
$$
sigmoid(x) = 1/(1+exp(-x))
$$
![image-20220101172105210](C:\Users\DAI FENGYUAN\AppData\Roaming\Typora\typora-user-images\image-20220101172105210.png)

Tanhå‡½æ•°
$$
tanh(x) = (1-exp(-2x))/(1+exp(-2x)
$$
![image-20220101172237134](C:\Users\DAI FENGYUAN\AppData\Roaming\Typora\typora-user-images\image-20220101172237134.png)

ReLUå‡½æ•° ï¼ˆrectified linear unitï¼‰  ç®—çš„å¿«ï¼Œç›¸æ¯”å‰é¢ä¸¤ä¸ªä¸ç”¨åšæŒ‡æ•°è¿ç®—
$$
Relu(x) = max(x,0)
$$
![image-20220101172424254](C:\Users\DAI FENGYUAN\AppData\Roaming\Typora\typora-user-images\image-20220101172424254.png)

### 4.1.2 è¶…å‚æ•°ï¼š

éšè—å±‚æ•°ï¼›

æ¯å±‚éšè—å±‚æ•°çš„å¤§å°ï¼›

## 4.2 å¤šå±‚æ„ŸçŸ¥æœºçš„ä»é›¶å®ç°

### 4.2.1 å¯¼å…¥åº“å¹¶åˆå§‹åŒ–è¾“å…¥å‚æ•°

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

### 4.2.2 å®šä¹‰å±‚å’Œæ¯å±‚çš„çš„æƒé‡

 å‚æ•°wä¸€å®šè¦ç”¨randnï¼Œåˆå§‹åŒ–æ—¶å…¨è®¾ç½®æˆ0ä¼šè®­ç»ƒä¸åŠ¨ ï¼ˆç›²çŒœæ˜¯åˆå§‹åŒ–ä¸º0æ—¶æ±‚å¯¼ä¼šå‡ºé”™ï¼‰

```python
num_inputs,num_outputs,num_hiddens = 784,10,256
W1 = nn.Parameter(torch.randn(num_inputs,num_hiddens,requires_grad = True)*0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens,requires_grad=True))
W2 = nn.Parameter(torch.randn(num_hiddens,num_outputs,requires_grad = True)*0.01)
b2 = nn.Parameter(torch.zeros(num_outputs,requires_grad=True))

params = [W1,b1,W2,b2]
```

æ²ç¥è¯´ä¸Šé¢çš„nn.Parameter(ï¼‰ä¸åŠ ä¹Ÿæ²¡å…³ç³»ï¼Œä¹‹å‰æ˜¯æ²¡åŠ çš„ã€‚ä½†æ˜¯æˆ‘è¯•äº†ä¸‹ï¼Œåœ¨è·‘åˆ°åé¢updaterä¼šæŠ¥é”™ï¼Œæˆ‘æƒ³åŸå› å¤§æ¦‚æ˜¯updaterç”¨çš„æ˜¯ torch.optim.SGDè€Œéè‡ªå·±å®šä¹‰çš„å‡½æ•°ã€‚æŠ¥é”™åŸå› å¦‚ä¸‹ï¼š

```python
# ValueError: can't optimize a non-leaf Tensor
```

### 4.2.3 å®šä¹‰reluï¼Œæ¨¡å‹å’Œloss

@è¡¨ç¤ºçŸ©é˜µä¹˜æ³•

```python
def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X,a)
    
def net(X):
    X = X.reshape((-1,num_inputs))
    H = relu(X @ W1+ b1)
    return (H@W2+b2)

loss = nn.CrossEntropyLoss()
```

### 4.2.4 è®­ç»ƒ

è®­ç»ƒå’Œä¹‹å‰ä¸€æ ·

```python
num_epoch ,lr =10,0.1
updater = torch.optim.SGD(params, lr=lr)
d2l.train_ch3(net,train_iter,test_iter,loss,num_epoch,updater)
```

## 4.3 å¤šå±‚æ„ŸçŸ¥æœºçš„ç®€æ´å®ç°

å¯¼å…¥åº“

```python
import torch
from torch import nn
from d2l import torch as d2l
```

### 4.3.1 å®šä¹‰æ¨¡å‹ï¼Œåˆå§‹åŒ–å‚æ•°

```python
net = nn.Sequential(nn.Flatten(),nn.Linear(784,256),nn.ReLU(),nn.Linear(256,10))

def init_weight(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)
    
net.apply(init_weight)
```

### 4.3.2 å®šä¹‰è¶…å‚æ•°ï¼Œlosså’Œupdaterå¹¶è®­ç»ƒ

```python
batch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss()
trainer = torch.optim.SGD(net.parameters(), lr=lr)

train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

## 4.4 æ¨¡å‹é€‰æ‹©ã€è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆ

### 4.4.1 è®­ç»ƒè¯¯å·®å’Œæ³›åŒ–è¯¯å·®

**è®­ç»ƒè¯¯å·®**ï¼šæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šçš„è¯¯å·®ï¼ˆä¸å…³å¿ƒçš„ï¼‰

**æ³›åŒ–è¯¯å·®**ï¼šæ¨¡å‹åœ¨æ–°æ•°æ®ä¸Šçš„è¯¯å·®ï¼ˆæˆ‘ä»¬å…³å¿ƒçš„ï¼‰

### 4.4.2 æ¨¡å‹é€‰æ‹©

**è®­ç»ƒæ•°æ®é›†**ï¼šè®­ç»ƒæ¨¡å‹å‚æ•°ï¼Œç”¨äºæ¯”è¾ƒæ¨¡å‹å’Œæ¨¡å‹

**éªŒè¯æ•°æ®é›†**ï¼šé€‰æ‹©æ¨¡å‹è¶…å‚æ•°ï¼Œç”¨äºæ¯”è¾ƒä¸€ä¸ªæ¨¡å‹ï¼Œä¸åŒè¶…å‚æ•°é€‰æ‹©çš„æƒ…å†µ

- å¦‚æ‹¿50%å‡ºæ¥ç”¨ä½œè®­ç»ƒæ•°æ®
- è·Ÿè®­ç»ƒæ•°æ®ä¸åŒï¼›ç±»ä¼¼å¹³å¸¸å°æµ‹éªŒï¼Œå‘¨è€ƒï¼›
- åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹

**æµ‹è¯•æ•°æ®é›†**ï¼šåªç”¨ä¸€æ¬¡çš„æ•°æ®é›†

- ç±»ä¼¼é«˜è€ƒï¼ˆQAQï¼‰

**K-åˆ™äº¤å‰éªŒè¯**ï¼š

åœ¨æ²¡æœ‰è¶³å¤Ÿå¤šçš„æ•°æ®æ—¶ä½¿ç”¨

- å°†è®­ç»ƒæ•°æ®åˆ†å‰²æˆkå—
- for i = 1ï¼Œ...ï¼ŒK
  - ä½¿ç”¨ç¬¬iå—ä½œä¸ºéªŒè¯é›†ï¼Œå…¶ä»–ä½œä¸ºè®­ç»ƒé›†ï¼Œæ‰§è¡ŒKæ¬¡æ¨¡å‹è®­ç»ƒå’ŒéªŒè¯ï¼›æ¯æ¬¡ç”¨ä¸åŒè¶…å‚æ•°çš„æ¨¡å‹
- æŠ¥å‘ŠKä¸ªéªŒè¯é›†è¯¯å·®çš„å¹³å‡

å¸¸ç”¨çš„Kä¸º5æˆ–10

### 4.4.3 è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆ

| åˆ—ï¼šæ¨¡å‹å®¹é‡ï¼›è¡Œï¼šæ•°æ® | ç®€å•   | å¤æ‚   |
| ---------------------- | ------ | ------ |
| ä½                     | æ­£å¸¸   | æ¬ æ‹Ÿåˆ |
| é«˜                     | è¿‡æ‹Ÿåˆ | æ­£å¸¸   |

**æ¨¡å‹å®¹é‡ï¼š**

- æ‹Ÿåˆå„ç§å‡½æ•°çš„èƒ½åŠ›
- ä½å®¹é‡çš„æ¨¡å‹éš¾ä»¥æ‹Ÿåˆè®­ç»ƒæ•°æ®
- é«˜å®¹é‡çš„æ¨¡å‹å¯ä»¥è®°ä½æ‰€æœ‰çš„è®­ç»ƒæ•°æ®

![image-20220102011721008](C:\Users\DAI FENGYUAN\AppData\Roaming\Typora\typora-user-images\image-20220102011721008.png)

æ¨¡å‹å®¹é‡è¯„ä¼°ï¼ˆä¸åŒç®—æ³•ä¸å¥½è¯„ä¼°ï¼Œåªé’ˆå¯¹åŒä¸€ä¸ªæ¨¡å‹ç§ç±»ï¼‰ï¼š

- å‚æ•°çš„ä¸ªæ•°
- å‚æ•°å€¼çš„é€‰æ‹©èŒƒå›´

## 4.5 æƒé‡è¡°é€€ weight decay

å¸¸è§çš„å¤„ç†è¿‡æ‹Ÿåˆçš„æ–¹æ³• ï¼šé™åˆ¶å‚æ•°å€¼çš„é€‰æ‹©èŒƒå›´
$$
\min{ lï¼ˆw,bï¼‰}    subject to ||w||^2 \leq \theta
$$

- é€šå¸¸ä¸é™åˆ¶åç§»bï¼ˆé™ä¸é™åˆ¶éƒ½å·®ä¸å¤šï¼‰
- æ›´å°çš„thetaæ„å‘³ç€æ›´å¼ºçš„æ­£åˆ™é¡¹

ä¸Šå¼ç­‰ä»·äºï¼š
$$
\min{l(w,b)}+\frac{\lambda}{2}*||w||^2
$$
è¶…å‚æ•°lambdaæ§åˆ¶äº†æ­£åˆ™é¡¹çš„é‡è¦ç¨‹åº¦

- lambda=0  æ— ä½œç”¨
- lambda->æ— ç©·ï¼Œw->0



å®è·µå†…å®¹å…¶å®å·®ä¸å¤šï¼Œå°±æ˜¯åœ¨è®¡ç®—lossæ—¶åŠ äº†ä¸€ä¸ªL1_penaltyï¼Œå†è®¡ç®—æ¢¯åº¦

## 4.6 ä¸¢å¼ƒæ³• Dropout

ä¹Ÿæ˜¯å¤„ç†è¿‡æ‹Ÿåˆçš„æ–¹æ³•ï¼ˆè·Ÿæ­£åˆ™é¡¹æ•ˆæœæ˜¯ä¸€æ ·çš„ï¼‰

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œåœ¨è®¡ç®—åç»­å±‚ä¹‹å‰å‘ç½‘ç»œçš„æ¯ä¸€å±‚æ³¨å…¥å™ªå£°ã€‚

**å¦‚ä½•æ³¨å…¥å™ªå£°ï¼Ÿ**

- æˆ‘ä»¬å¸Œæœ›åŠ å…¥å™ªå£°è¿‡åhçš„æ•°å­¦æœŸæœ›ä¸å˜ï¼Œå¯¹hä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ åšå¦‚ä¸‹æ‰°åŠ¨ï¼›å…¶ä¸­pæ˜¯è¶…å‚æ•°

![image-20220102162654080](C:\Users\DAI FENGYUAN\AppData\Roaming\Typora\typora-user-images\image-20220102162654080.png)

- åªåœ¨è®­ç»ƒä¸­è¿›è¡Œï¼šå½±å“æ¨¡å‹å‚æ•°çš„æ›´æ–°ï¼›

- è¾“å‡ºæ—¶å¹¶ä¸ç”¨dropoutï¼›

## 4.8 æ•°å€¼ç¨³å®šæ€§å’Œæ¨¡å‹åˆå§‹åŒ–

nanï¼šä¸€ä¸ªæ•°é™¤0ï¼›æ•°å€¼å¤ªå¤§

inf  ï¼šlrè°ƒå¤ªå¤§äº†

å»ºè®®lrå¾€å°é‡Œè°ƒï¼Œç›´åˆ°èƒ½æ­£ç¡®åœ°å‡ºä¸€äº›å€¼

### 4.8.1 æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸

å¯¹ä¸€ä¸ªLå±‚çš„MLPï¼Œè‹¥Lå¤ªå¤§ï¼Œæœ€åæ±‚å¯¼çš„ç»“æœå¯èƒ½èµ°å‘æç«¯ï¼šï¼ˆè¿™éƒ¨åˆ†æœ‰å…³çŸ©é˜µæ±‚å¯¼ï¼ŒåŸç†å…¶å®çœ‹ä¸å¤§æ‡‚ğŸ˜ï¼‰

- æ¢¯åº¦çˆ†ç‚¸
- æ¢¯åº¦æ¶ˆå¤±

å› æ­¤ï¼Œè¿™ç§æ¨¡å‹è¦ä¹ˆå¯¹å­¦ä¹ ç‡å¾ˆæ•æ„Ÿï¼Œå¯èƒ½éœ€è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ–­è°ƒæ•´å­¦ä¹ ç‡ï¼›è¦ä¹ˆä¸ç®¡å¦‚ä½•é€‰æ‹©å­¦ä¹ ç‡ï¼Œè®­ç»ƒéƒ½æ²¡æœ‰è¿›å±•

### 4.8.2 å¦‚ä½•è®©è®­ç»ƒæ›´åŠ ç¨³å®š

**è®©ä¹˜æ³•å˜åŠ æ³•**

- ResNetï¼ŒLSTM

**å½’ä¸€åŒ–**

- æ¢¯åº¦å½’ä¸€åŒ–ï¼Œæ¢¯åº¦å‰ªè£

**åˆç†çš„æƒé‡åˆå§‹å’Œæ¿€æ´»å‡½æ•°**   -> ä¸ºäº†æå‡æ•°å€¼ç¨³å®šæ€§

ç›®æ ‡ï¼šMLPä¸­æ¯ä¸€å±‚è¾“å‡ºçš„æ–¹å·®å’ŒæœŸæœ›ä¸€è‡´

å› æ­¤åˆå§‹åŒ–æƒé‡æ—¶ï¼Œè¦æ±‚æ»¡è¶³

å‰å‘ä¼ æ’­
$$
n_{t-1}Î³_t = 1
$$
åå‘ä¼ æ’­
$$
n_{t}Î³_t = 1
$$
å…¶ä¸­nä¸ºç¬¬tå±‚çš„è¾“å…¥æ•°æ®ä¸ªæ•°ï¼ŒÎ³ä¸ºç¬¬tå±‚æƒé‡çš„æ–¹å·®

ä½†æ˜¯å¾ˆéš¾åŒæ—¶æ»¡è¶³ä¸Šè¿°ä¸¤ä¸ªæ¡ä»¶ï¼ˆé™¤éè¾“å…¥ä¸ªæ•°ç­‰äºè¾“å‡ºä¸ªæ•°ï¼‰

**è§£å†³æ–¹æ³•ï¼š**

Xavierä½¿å¾—ä¸Šä¸¤å¼çš„å‡å€¼ä¸º1ï¼Œå³ Î³{t} = 2/(n{t-1}+n{t})

å› æ­¤åˆå§‹åŒ–æƒé‡æ—¶ï¼Œè¦æ±‚

<img src="C:\Users\DAI FENGYUAN\AppData\Roaming\Typora\typora-user-images\image-20220103185707893.png" alt="image-20220103185707893" style="zoom: 50%;" />

æˆ–è€…

<img src="C:\Users\DAI FENGYUAN\AppData\Roaming\Typora\typora-user-images\image-20220103185748679.png" alt="image-20220103185748679" style="zoom:50%;" />

æ£€æŸ¥æ¿€æ´»å‡½æ•°ï¼š

- tanhå’Œreluéƒ½å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šè¿‘ä¼¼æˆ–è€…ç›´æ¥çœ‹ä½œæ­£æ¯”ä¾‹å‡½æ•°
- sigmoidéœ€è¦è°ƒæ•´ï¼š 4*sigmoid - 2

## 4.9 å®æˆ˜Kaggleæ¯”èµ›

**Qï¼šå°†ç±»åˆ«å˜é‡è½¬æ¢æˆä¼ªå˜é‡æ—¶å†…å­˜çˆ†äº†å’‹åŠï¼Ÿ**

Aï¼šè¦ä¹ˆæ”¹ç”¨ç¨€ç–çŸ©é˜µï¼›è¦ä¹ˆä¸ç”¨one hotï¼Œç”¨å…¶ä»–æ–¹æ³•æˆ–è€…å¿½ç•¥0.0ï¼›

**Qï¼šè®­ç»ƒæ—¶losså›¾åƒæŠ–åŠ¨å‰å®³ï¼Ÿ**

Aï¼šlrå¤ªå¤§äº†æˆ–è€…æ˜¯batchå¤ªå°

# 5 æ·±åº¦å­¦ä¹ è®¡ç®—

## 5.1 å±‚å’Œå—

ä»»ä½•ä¸€ä¸ªå±‚æˆ–ç¥ç»ç½‘ç»œéƒ½æ˜¯moduleçš„å­ç±»ï¼›

**è‡ªå®šä¹‰å—MLPï¼š**

```python
class MLP(nn.Module):
    # ç”¨æ¨¡å‹å‚æ•°å£°æ˜å±‚ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬å£°æ˜ä¸¤ä¸ªå…¨è¿æ¥çš„å±‚
    def __init__(self):
        # è°ƒç”¨MLPçš„çˆ¶ç±»Moduleçš„æ„é€ å‡½æ•°æ¥æ‰§è¡Œå¿…è¦çš„åˆå§‹åŒ–ã€‚
        # è¿™æ ·ï¼Œåœ¨ç±»å®ä¾‹åŒ–æ—¶ä¹Ÿå¯ä»¥æŒ‡å®šå…¶ä»–å‡½æ•°å‚æ•°ï¼Œä¾‹å¦‚æ¨¡å‹å‚æ•°paramsï¼ˆç¨åå°†ä»‹ç»ï¼‰
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # éšè—å±‚
        self.out = nn.Linear(256, 10)  # è¾“å‡ºå±‚

    # å®šä¹‰æ¨¡å‹çš„å‰å‘ä¼ æ’­ï¼Œå³å¦‚ä½•æ ¹æ®è¾“å…¥Xè¿”å›æ‰€éœ€çš„æ¨¡å‹è¾“å‡º
    def forward(self, X):
        # æ³¨æ„ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ReLUçš„å‡½æ•°ç‰ˆæœ¬ï¼Œå…¶åœ¨nn.functionalæ¨¡å—ä¸­å®šä¹‰ã€‚
        return self.out(F.relu(self.hidden(X)))
```

ç­‰ä»·äºä¸‹é¢ä»£ç ï¼š

```python
import torch
from torch import nn
from torch.nn import functional as F

net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))

X = torch.rand(2, 20)
net(X)
```

**å®šä¹‰é¡ºåºå—ï¼š**

self_modulesæ˜¯ä¸€ä¸ªordered dictionaryÂ·

```python
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # è¿™é‡Œï¼Œmoduleæ˜¯Moduleå­ç±»çš„ä¸€ä¸ªå®ä¾‹ã€‚æˆ‘ä»¬æŠŠå®ƒä¿å­˜åœ¨'Module'ç±»çš„æˆå‘˜
            # å˜é‡_modulesä¸­ã€‚moduleçš„ç±»å‹æ˜¯OrderedDict
            self._modules[str(idx)] = module

    def forward(self, X):
        # OrderedDictä¿è¯äº†æŒ‰ç…§æˆå‘˜æ·»åŠ çš„é¡ºåºéå†å®ƒä»¬
        for block in self._modules.values():
            X = block(X)
        return X
```

é€šè¿‡å®šä¹‰forwardï¼Œå¯ä»¥è‡ªå®šä¹‰ä¹±ä¸ƒå…«ç³Ÿçš„å—ï¼ˆå¯ä»¥åšå¾—å¾ˆçµæ´»ï¼‰ï¼š

```python
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # ä¸è®¡ç®—æ¢¯åº¦çš„éšæœºæƒé‡å‚æ•°ã€‚å› æ­¤å…¶åœ¨è®­ç»ƒæœŸé—´ä¿æŒä¸å˜
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20)

    def forward(self, X):
        X = self.linear(X)
        # ä½¿ç”¨åˆ›å»ºçš„å¸¸é‡å‚æ•°ä»¥åŠreluå’Œmmå‡½æ•°
        X = F.relu(torch.mm(X, self.rand_weight) + 1)
        # å¤ç”¨å…¨è¿æ¥å±‚ã€‚è¿™ç›¸å½“äºä¸¤ä¸ªå…¨è¿æ¥å±‚å…±äº«å‚æ•°
        X = self.linear(X)
        # æ§åˆ¶æµ
        while X.abs().sum() > 1:
            X /= 2
        return X.sum()
```

ä¹Ÿå¯ä»¥å¥—å¨ƒï¼š

```python
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),
                                 nn.Linear(64, 32), nn.ReLU())
        self.linear = nn.Linear(32, 16)

    def forward(self, X):
        return self.linear(self.net(X))

chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())
chimera(X)
```

## 5.2 å‚æ•°ç®¡ç†

### 5.2.1 è®¿é—®å‚æ•°

**é€šè¿‡ä¸‹é¢ä»£ç è®¿é—®ç¬¬äºŒå±‚çš„æ¨¡å‹å‚æ•°ï¼š**

```python
print(net[2].state_dict())
```

ç»“æœï¼š

```html
OrderedDict([('weight', tensor([[-0.2680, -0.3387,  0.0259, -0.0591,  0.1884,  0.2721,  0.1892,  0.2496]])), ('bias', tensor([0.3248]))])
```

**ä¸€æ¬¡æ€§è®¿é—®æ‰€æœ‰å‚æ•°ï¼š**

```python
print([(name, param.shape) for name, param in net[0].named_parameters()])
print([(name, param.shape) for name, param in net.named_parameters()])
```

ç»“æœï¼š

```html
('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))
('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))
```

è®¿é—®æœ€åä¸€å±‚çš„åç§»ï¼š

```python
net.state_dict()['2.bias'].data
```

ç»“æœï¼š

```html
tensor([0.3248])
```

å¯ä»¥ç”¨print(net)æ¥æ‰“å°ç½‘ç»œç»“æ„

### 5.2.2 å†…ç½®åˆå§‹åŒ–

applyï¼šå¯¹æ¨¡å‹æ‰€æœ‰çš„å±‚ä½¿ç”¨æŸå‡½æ•°

å¯¹ç±»ä¼¼normal\_ï¼Œzeros\_çš„å‡½æ•°,åœ¨å‡½æ•°åé¢åŠ \_è¡¨ç¤ºæ›¿æ¢

**åŸºæœ¬åˆå§‹åŒ–**

```python
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init.zeros_(m.bias)
        
net.apply(init_normal)
```

**å¯¹æŸäº›å—åº”ç”¨ä¸åŒçš„åˆå§‹åŒ–**

```python
def xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)
def init_42(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 42)

net[0].apply(xavier)
net[2].apply(init_42)
```

**è‡ªå®šä¹‰åˆå§‹åŒ–**

ç›®æ ‡ï¼š![image-20220104162944403](C:\Users\DAI FENGYUAN\AppData\Roaming\Typora\typora-user-images\image-20220104162944403.png)

```python
def my_init(m):
    if type(m) == nn.Linear:
        print("Init", *[(name, param.shape)
                        for name, param in m.named_parameters()][0])
        nn.init.uniform_(m.weight, -10, 10)
        m.weight.data *= m.weight.data.abs() >= 5

net.apply(my_init)
```

**æš´åŠ›æ–¹æ³•ï¼š**

```
net[0].weight.data[:] += 1
net[0].weight.data[0, 0] = 42
```

### 5.2.3 å‚æ•°ç»‘å®š

å¸Œæœ›åœ¨å¤šä¸ªå±‚ä¹‹é—´å…±äº«å‚æ•°

```python
# æˆ‘ä»¬éœ€è¦ç»™å…±äº«å±‚ä¸€ä¸ªåç§°ï¼Œä»¥ä¾¿å¯ä»¥å¼•ç”¨å®ƒçš„å‚æ•°
shared = nn.Linear(8, 8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                    shared, nn.ReLU(),
                    shared, nn.ReLU(),
                    nn.Linear(8, 1))
net(X)
# æ£€æŸ¥å‚æ•°æ˜¯å¦ç›¸åŒ
print(net[2].weight.data[0] == net[4].weight.data[0])
net[2].weight.data[0, 0] = 100
# ç¡®ä¿å®ƒä»¬å®é™…ä¸Šæ˜¯åŒä¸€ä¸ªå¯¹è±¡ï¼Œè€Œä¸åªæ˜¯æœ‰ç›¸åŒçš„å€¼
print(net[2].weight.data[0] == net[4].weight.data[0])
```

è¾“å‡ºå…¨æ˜¯True

## 5.3 è‡ªå®šä¹‰å±‚

nn.moduleä¸­ï¼Œç›´æ¥è°ƒç”¨net(X)ç­‰ä»·äºnet.forward(X)ï¼Œæ‰€ä»¥å®šä¹‰forwardå°±æ˜¯å®šä¹‰è¿ç®—

### 5.3.1 ä¸å¸¦å‚æ•°çš„å±‚

ä¸€ä¸ªå±•ç¤ºæ¯ä¸ªå…ƒç´ çš„ç¦»å‡å€¼çš„ä¾¿å®œç¨‹åº¦çš„å±‚

```python
import torch
import torch.nn.functional as F
from torch import nn


class CenteredLayer(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, X):
        return X - X.mean()
```

```python
layer = CenteredLayer()
layer(torch.FloatTensor([1, 2, 3, 4, 5]))
```

```html
tensor([-2., -1.,  0.,  1.,  2.])
```

### 5.3.2 å¸¦å‚æ•°çš„å±‚

å…¶å®å°±æ˜¯çº¿æ€§å±‚

```python
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))
    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return F.relu(linear)
```

printä¸€ä¸‹ï¼š

```python
linear = MyLinear(5, 3)
linear.weight
```

```html
Parameter containing:
tensor([[ 2.2578, -0.4571, -1.4185],
        [ 1.6469, -0.4779,  1.0448],
        [ 0.9764,  0.4048,  0.6039],
        [ 0.9259,  1.2831, -0.4907],
        [ 1.1768, -0.2245,  1.2883]], requires_grad=True)
```

ä½¿ç”¨è‡ªå®šä¹‰å±‚ï¼ˆå…¶å®è·Ÿtorché‡Œé¢çš„linearä¸€æ ·ï¼‰

```python
linear(torch.rand(2, 5))
```

ç»“æœï¼š

```html
tensor([[0.9983, 1.5834, 0.1999],
        [2.7293, 0.3060, 1.1866]])
```

å¯ä»¥æ”¾Sequentialé‡Œé¢

```python
net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))
net(torch.rand(2, 64))
```

ç»“æœ

```html
tensor([[0.],
        [0.]])
```

## 5.4 è¯»å†™æ–‡ä»¶

### 5.4.1 åŠ è½½å’Œä¿å­˜å¼ é‡

è°ƒç”¨loadå’Œsaveåˆ†åˆ«è¯»å†™

ä¿å­˜çš„å…ƒç´ xå¯ä»¥æ˜¯å¼ é‡ï¼Œåˆ—è¡¨ä»¥åŠå­—å…¸

```python
import torch
from torch import nn
from torch.nn import functional as F

x = torch.arange(4)
torch.save(x, 'x-file')
```

```python
x2 = torch.load('x-file')
```

### 5.4.2 åŠ è½½å’Œä¿å­˜æ¨¡å‹å‚æ•°

åªèƒ½å­˜æ¨¡å‹çš„æƒé‡å’Œåç§»

```python
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.output = nn.Linear(256, 10)

    def forward(self, x):
        return self.output(F.relu(self.hidden(x)))

net = MLP()
X = torch.randn(size=(2, 20))
Y = net(X)

torch.save(net.state_dict(), 'mlp.params')
```

loadå‰éœ€è¦çŸ¥é“åŸå…ˆçš„æ¨¡å‹ç»“æ„

```python
clone = MLP()
clone.load_state_dict(torch.load('mlp.params'))
clone.eval()  # Sets the module in evaluation mode. åœ¨è¿™é‡Œåªæ˜¯æƒ³printä¸€ä¸‹
```

## 5.5 ä½¿ç”¨GPU

### 5.5.1 è®¾å¤‡

æŸ¥çœ‹è®¾å¤‡ä¿¡æ¯ï¼š

```python
!nvidia-smi
```

æŸ¥è¯¢å¯ç”¨gpuæ•°é‡

```python
torch.cuda.device_count()
```

ä¸¤ä¸ªå‡½æ•°ï¼Œæ–¹ä¾¿æˆ‘ä»¬åœ¨å­˜åœ¨GPUå’Œä¸å­˜åœ¨çš„æ—¶å€™éƒ½å¯ä»¥ä½¿ç”¨

```python
def try_gpu(i=0):  #@save
    """å¦‚æœå­˜åœ¨ï¼Œåˆ™è¿”å›gpu(i)ï¼Œå¦åˆ™è¿”å›cpu()"""
    if torch.cuda.device_count() >= i + 1:
        return torch.device(f'cuda:{i}')
    return torch.device('cpu')

def try_all_gpus():  #@save
    """è¿”å›æ‰€æœ‰å¯ç”¨çš„GPUï¼Œå¦‚æœæ²¡æœ‰GPUï¼Œåˆ™è¿”å›[cpu(),]"""
    devices = [torch.device(f'cuda:{i}')
             for i in range(torch.cuda.device_count())]
    return devices if devices else [torch.device('cpu')]

```



### 5.5.2 å¼ é‡å’ŒGPU

æŸ¥è¯¢å¼ é‡æ‰€åœ¨è®¾å¤‡ï¼Œé»˜è®¤åœ¨cpuä¸Šï¼Œæ³¨æ„æ— è®ºä½•æ—¶æˆ‘ä»¬è¦å¯¹å¤šä¸ªé¡¹è¿›è¡Œæ“ä½œï¼Œ å®ƒä»¬éƒ½å¿…é¡»åœ¨åŒä¸€ä¸ªè®¾å¤‡ä¸Šï¼›

```python
x = torch.tensor([1, 2, 3])
x.device
```

è¿”å›device(type='cpu')ï¼›

å°†å¼ é‡å­˜å‚¨åœ¨GPUä¸Š

```python
X = torch.ones(2, 3, device=try_gpu())
X
```

è¿”å›

```python
tensor([[1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
```

å°†cpuä¸Šçš„tensorå˜é‡xåœ¨gpuä¸­å¤åˆ¶ä¸€ä»½

```python
Z = X.cuda(0)  # Z = X.cuda()
X + Z 
```

### 5.5.3 ç¥ç»ç½‘ç»œå’ŒGPU

å°†æ¨¡å‹æ”¾åˆ°GPUä¸Š

```python
net = nn.Sequential(nn.Linear(3, 1))
net = net.to(device=try_gpu())
```

