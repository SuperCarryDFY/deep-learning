DIVE INTO DEEP LEARNING

# 2.预备知识

## 2.1 数据操作

**创建数据**

```python
torch.arange(12)         #类似range
torch.zeros((2,3,4))
torch.ones((2,3,4))
torch.randn((3,4))       #随机函数
torch.tensor([[2,1,4,3],[1,2,3,4],[4,3,2,1]])  #列表转张量
```

**数据处理**

```python
x+y
x-y
x*y
x/y
x**y               #y中元素是x的幂次
torch.exp(x)       #对x的元素逐个exp
X = x.reshape(3,4) #改变形状，大小不变。可以用-1代表懒得算 如x.reshape(-1,4)
```

**数据信息**

```python
x.shape            #形状
x.numel()          #大小
```

## 2.2 数据预处理

pandas

对缺失的，表现为NaN值，可以用pd.get_dummies方法

```python
inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]
inputs = inputs.fillna(inputs.mean()) # 对数值域的元素为NaN的，填充为其他数值域的均值
print(inputs)
```

|      | NumRooms | Alley |
| :--: | :------: | :---: |
|  0   |   3.0    | Pave  |
|  1   |   2.0    |  NaN  |
|  2   |   4.0    |  NaN  |
|  3   |   3.0    |  NaN  |

```python
inputs = pd.get_dummies(inputs, dummy_na=True)
inputs
```

|      | NumRooms | Alley_Pave | Alley_nan |
| :--: | :------: | :--------: | :-------: |
|  0   |   3.0    |     1      |     0     |
|  1   |   2.0    |     0      |     1     |
|  2   |   4.0    |     0      |     1     |
|  3   |   3.0    |     0      |     1     |

## 2.3 线性代数

**矩阵转置**： 延对角线反转

**对称矩阵**： B ==B.T

**矩阵创建**

```python
x = torch.arange(24).reshape(2,3,4)
```

**矩阵对应元素相乘（Hadamard product）**

```python
A * B
```

**降维**    sum函数中的axis，0为按行求和（每一列的求和作为一个值），1为按列求和，以此类推

​			keepdims=True，使被求和的维度的大小降为1。即按原本维度输出。（默认的话，无论axis为多少，都是按照行列开始输出的） 用于广播机制。

**点积**

```python
torch.dot(x,y)  #等价于下式
torch.sum(x*y)
```

**向量积**

```python
torch.mm(A,B)
```

**范数**

```python
torch.norm(u)          #长度，各元素平方和的平方根
torch.abs(u).sum()     #L1范数，各元素绝对值的和
```

## 2.5 自动微分

自变量x，因变量y

**将x的requires_grad设置为True，自动记录x的梯度**

```python
x = torch.arange(4.0)
y = 2 * torch.dot(x,x)
x.requires_grad_(True) # 等价于 x=torch.arange(4.0,requires_grad=True)
```

**调用反向传播函数计算y关于x的梯度**

```python
y.backward()
x.grad
```

结果：tensor([ 0.,  4.,  8., 12.])

# 3.线性神经网络

## 3.1 线性回归

线性模型：单层神经网络；

对n维输入加权，加偏差；

用平方损失来衡量预测和真实值的差异；

两个超参数hyperparameters：学习速率和批量大小（整个在训练集上算梯度太贵了，需要每次采样b个样本来近似）；

模型参数：模型里的参数，如w，b；

深度学习的默认求解算法：小批量随机梯度下降

## 3.2 线性回归从零开始实现

### 3.2.1 生成样本集

```python
def synthetic_data(w,b,num_examples):
    x = torch.normal(0,1,(num_examples,len(w)))
    y = torch.matmul(x,w)+b
    y += torch.normal(0,0.01,y.shape)
    return x,y.reshape((-1,1))

true_w = torch.tensor([2,-3.4])
true_b = 4.2
features,labels = synthetic_data(true_w,true_b,1000)
```

torch.matmul   # Matrix product of two tensors

torch.mm          # Supports strided and sparse 2-D tensors as inputs, autograd with respect to strided inputs. not support broadcasting

**查看样本集：**

```python
d2l.set_figsize()
d2l.plt.scatter(features[:,1].detach().numpy(), 
               labels.detach().numpy(), 1);
```

detach() # 返回一个新的tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置,不同之处只是requires_grad为false，得到的这个tensor永远不需要计算其梯度，不具有grad。

### 3.2.2 生成小批量数据

```python
def data_iter(batch_size,features,labels):
    num_examples = len(labels)
    indices = list(range(num_examples))
    random.shuffle(indices)
    for i in range(0,num_examples,batch_size):
        batch_indices = torch.tensor(indices[i,min(i+batch_size,num_examples)])
        yield features[batch_indices],labels[batch_indices]
```

*torch里面的 tensor竟然支持张量嵌套0.0 无敌了👀* 

> *比如：* 
>
> *a = torch.tensor([1,2,3,4,5,6,7])*
> *b = torch.tensor([2,3,4])*
> *那么 a[b]的值为  tensor([3, 4, 5])*

### 3.2.3 定义初始化模型参数和模型

```python
w = torch.normal(0,0.01,size=(2,1),requires_grad=True)
b = torch.zeros(1,requires_grad=True)

def linreg(X,w,b):
    return torch.matmul(X,w)+b
```

### 3.2.4 定义损失函数

```python
def squared_loss(y_hat,y):
    return(y_hat - y.reshape(y_hat.shape))**2/2
```

### 3.2.5 定义优化算法

```python
def sgd(params,lr,batch_size):
    with torch.no_grad():
        for param in params:
            param -= lr* param.grad / batch_size
            param.grad.zero_()
```

### 3.2.6 训练

```python
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss

for epoch in range(num_epochs):
    for X,y in data_iter(batch_size,features,labels):
        l = loss(net(X,w,b),y)
        l.sum().backward()
        sgd([w,b],lr,batch_size)
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
```

## 3.3 线性回归的简洁实现

初始操作

```python
import torch
from torch.utils import data
from d2l import torch as d2l repe
import numpy as np

true_w = torch.tensor([2,-3.4])
true_b = 4.2
features,labels = d2l.synthetic_data(true_w,true_b,1000)
```

### 3.3.1 返回生成并返回小批量数据

```python
def load_array(data_arrays,batch_size,is_train=True):
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset,batch_size,shuffle=is_train)

batch_size = 10
data_iter = load_array((features,labels),batch_size)

```

### 3.3.2 定义层

```python
from torch import nn 

net = nn.Sequential(nn.Linear(2,1))
```

nn ：neural network 神经网络；

指定输入维度为2，输出维度为1；

可以直接用 net = nn.Linear(2,1)，因为线性回归是单层的；

### 3.3.3 初始化参数模型

```python
net[0].weight.data.normal_(0,0.01)
net[0].bias.data.fill_(0)
```

**其实也可以不用设置**

net[0]： 访问net的第0层；

访问权重的data，将其设为正太分布；

将bias设为0；

### 3.3.4 设置loss和SGD

SGD（stochastic gradient descent）随机梯度下降

```python
loss = nn.MSELoss()
trainer = torch.optim.SGD(net.parameters(),lr=0.03)
```

net.parameters()为 线性回归层中的所有参数（w和b）

### 3.3.5 训练

```python
num_epochs = 3
for epoch in range(num_epochs):
    for X ,y in data_iter:
        l = loss(net(X),y)
        trainer.zero_grad()
        l.backward()
        trainer.step()
    l = loss(net(features),labels)
    print(f'epoch {epoch + 1}, loss {l:f}')
```

## 3.4 softmax 回归

回归 vs 分类

- 回归估计一个连续值
- 分类预测一个离散类别

对softmax 输入X，输出y_hat（一维张量，sum为1），操作为softmax，其中o是未规范化的预测

![image-20211230193114538](C:\Users\DAI FENGYUAN\AppData\Roaming\Typora\typora-user-images\image-20211230193114538.png)

将y_hat和真实值y的区别作为损失。下式说明回归问题只关心对真实值的预测概率。

$$
l(y,y') = - \sum_{j=1}^{q}{y_jlogy_j})
$$
其梯度恰好是真实概率和预测概率的差值 ***（问题：这怎么算的？）***

![image-20211230194135919](C:\Users\DAI FENGYUAN\AppData\Roaming\Typora\typora-user-images\image-20211230194135919.png)

## 3.5 图像分类数据集

引入的一些库

```python
%matplotlib inline
import torch
import torchvision
from torch.utils import data
from torchvision import transforms
from d2l import torch as d2l

d2l.use_svg_display()
```

下载数据集

```python
# 通过ToTensor实例将图像数据从PIL类型变换成32位浮点数格式，
# 并除以255使得所有像素的数值均在0到1之间
trans = transforms.ToTensor()
mnist_train = torchvision.datasets.FashionMNIST(
    root="../data", train=True, transform=trans, download=True)
mnist_test = torchvision.datasets.FashionMNIST(
    root="../data", train=False, transform=trans, download=True)
```

对数据集的说明：

- mnist_train是训练集，mnist_test是测试集

- ```python
  len(mnist_train), len(mnist_test)
  ```

  ```
  (60000, 10000)
  ```

- mnist_train[i]是一个元组，第一个元素是shape为([1,28,28])的通道数为1，高度和宽度均为28像素的图像，第二个shape是一个int，指明该图像的类型

可视化数据    ***其实不是很理解***

```python
def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save
    """绘制图像列表"""
    figsize = (num_cols * scale, num_rows * scale)
    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)
    axes = axes.flatten()
    for i, (ax, img) in enumerate(zip(axes, imgs)):
        if torch.is_tensor(img):
            # 图片张量
            ax.imshow(img.numpy())
        else:
            # PIL图片
            ax.imshow(img)
        ax.axes.get_xaxis().set_visible(False)
        ax.axes.get_yaxis().set_visible(False)
        if titles:
            ax.set_title(titles[i])
    return axes

def get_fashion_mnist_labels(labels):  #@save
    """返回Fashion-MNIST数据集的文本标签"""
    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
    return [text_labels[int(i)] for i in labels]

X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))
show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y));
```

## 3.6 softmax回归的从零开始实现

这节真的有点难啊

### 3.6.1 导入数据，并初始化输入参数

```python
import torch
from IPython import display
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

```python
num_inputs = 784
num_outputs = 10

W = torch.normal(0,0.01,size = (num_inputs,num_outputs),requires_grad = True)
b = torch.zeros(num_outputs,requires_grad = True)
```

### 3.6.2 定义softmax以及模型

```python
def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1,keepdim=True)
    return X_exp / partition

def net(X):
    return softmax(torch.matmul(X.reshape(-1,W.shape[0],W)+b))
```

### 3.6.3 定义loss函数

注：pytorch的torch嵌套还可以这么用

```python
y = torch.tensor([0, 2])
y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])
y_hat[[0,1],y]
```

其结果为：tensor([0.1000, 0.5000])  卧槽离谱`(*>﹏<*)′

实现交叉熵损失函数，公式如下（y'即为y_hat）
$$
l(y,y') = - \sum_{j=1}^{q}{y_jlogy_j})
$$
代码如下

```python
def cross_entropy(y_hat, y):
    return -torch.log(y_hat[range(len(y_hat)),y])
```

### 3.6.4 分类精度

这部分有点复杂 我有点懵。。。

```python
def accuracy(y_hat,y):
    '''计算预测正确的数量'''
    if len(y_hat.shape)>1 and y_hat.shape[1]>1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    return float(cmp.type(y.dtype).sum())
```

```python
def evaluate_accuracy(net, data_iter):  #@save
    """计算在指定数据集上模型的精度"""
    if isinstance(net, torch.nn.Module):
        net.eval()  # 将模型设置为评估模式,不用计算梯度，j
    metric = Accumulator(2)  # 正确预测数、预测总数
    with torch.no_grad():
        for X, y in data_iter:
            metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]
```

其中accumulator是一个累加器，其定义如下

```python
class Accumulator:  #@save
    """在n个变量上累加"""
    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
```

### 3.6.5 训练

每个epoch的训练：此训练包含了后面softmox简洁实现

```python
def train_epoch_ch3(net, train_iter, loss, updater):  #@save
    """训练模型一个迭代周期（定义见第3章）"""
    # 将模型设置为训练模式
    if isinstance(net, torch.nn.Module):
        net.train()
    # 训练损失总和、训练准确度总和、样本数
    metric = Accumulator(3)
    for X, y in train_iter:
        # 计算梯度并更新参数
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            # 使用PyTorch内置的优化器和损失函数
            updater.zero_grad()
            l.sum().backward()
            updater.step()
        else:
            # 使用定制的优化器和损失函数
            l.sum().backward()
            updater(X.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # 返回训练损失和训练精度
    return metric[0] / metric[2], metric[1] / metric[2]
```

后面用matplotlib画了一个图，用于可视化，就不贴代码了0.0

训练模型

```python
def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save
    """训练模型（定义见第3章）"""
    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                        legend=['train loss', 'train acc', 'test acc'])
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        animator.add(epoch + 1, train_metrics + (test_acc,))
    train_loss, train_acc = train_metrics
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc
```

最后定义updater（就是每次沿梯度更新一下parameters的值，）

```python
lr = 0.1

def updater(batch_size):
    return d2l.sgd([W, b], lr, batch_size)
```

然后开始训练

```python
num_epochs = 15
train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)
```

## 3.7 softmax回归的简洁实现

### 3.7.1 导入一些库

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter,test_iter = d2l.load_data_fashion_mnist(batch_size)
```

### 3.7.2 初始化模型参数

在线性层前定义了展平层（flatten），来调整网络输入的形状（类似从零实现的reshape）

nn.init.normal_ ：在对线性层中的每一个权重都设置成均值为0，默认为0，方差为0.01的随机值（好像后面有讲究的）

net.apply ：对每一个layer都call一次

```python
net = nn.Sequential(nn.Flatten(),nn.Linear(784,10))

def init_weight(m):   
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight,std=0.01)  
net.apply(init_weight)
```

### 3.7.3 loss函数和训练方法

nn.CrossEntropyLoss() ：交叉熵损失函数

```python
loss = nn.CrossEntropyLoss()
trainer = torch.optim.SGD(net.parameters(),lr=0.01)
```

### 3.7.4 训练

没啥好说的，用上一节的函数就完事了

```python
num_epochs = 10
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

# 4 多层感知机 MLP(Multilayer Perceptron)

单层感知机不能拟合XOR（异或）函数，只能分割线性面

- 解决方法：用多层感知机

多层感知机不加激活函数的话，还是线性回归

## 4.1 多层感知机

### 4.1.1 激活函数

Sigmoid函数 
$$
sigmoid(x) = 1/(1+exp(-x))
$$
![image-20220101172105210](C:\Users\DAI FENGYUAN\AppData\Roaming\Typora\typora-user-images\image-20220101172105210.png)

Tanh函数
$$
tanh(x) = (1-exp(-2x))/(1+exp(-2x)
$$
![image-20220101172237134](C:\Users\DAI FENGYUAN\AppData\Roaming\Typora\typora-user-images\image-20220101172237134.png)

ReLU函数 （rectified linear unit）  算的快，相比前面两个不用做指数运算
$$
Relu(x) = max(x,0)
$$
![image-20220101172424254](C:\Users\DAI FENGYUAN\AppData\Roaming\Typora\typora-user-images\image-20220101172424254.png)

### 4.1.2 超参数：

隐藏层数；

每层隐藏层数的大小；

## 4.2 多层感知机的从零实现

### 4.2.1 导入库并初始化输入参数

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

### 4.2.2 定义层和每层的的权重

 参数w一定要用randn，初始化时全设置成0会训练不动 （盲猜是初始化为0时求导会出错）

```python
num_inputs,num_outputs,num_hiddens = 784,10,256
W1 = nn.Parameter(torch.randn(num_inputs,num_hiddens,requires_grad = True)*0.01)
b1 = nn.Parameter(torch.zeros(num_hiddens,requires_grad=True))
W2 = nn.Parameter(torch.randn(num_hiddens,num_outputs,requires_grad = True)*0.01)
b2 = nn.Parameter(torch.zeros(num_outputs,requires_grad=True))

params = [W1,b1,W2,b2]
```

沐神说上面的nn.Parameter(）不加也没关系，之前是没加的。但是我试了下，在跑到后面updater会报错，我想原因大概是updater用的是 torch.optim.SGD而非自己定义的函数。报错原因如下：

```python
# ValueError: can't optimize a non-leaf Tensor
```

### 4.2.3 定义relu，模型和loss

@表示矩阵乘法

```python
def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X,a)
    
def net(X):
    X = X.reshape((-1,num_inputs))
    H = relu(X @ W1+ b1)
    return (H@W2+b2)

loss = nn.CrossEntropyLoss()
```

### 4.2.4 训练

训练和之前一样

```python
num_epoch ,lr =10,0.1
updater = torch.optim.SGD(params, lr=lr)
d2l.train_ch3(net,train_iter,test_iter,loss,num_epoch,updater)
```

## 4.3 多层感知机的简洁实现

导入库

```python
import torch
from torch import nn
from d2l import torch as d2l
```

### 4.3.1 定义模型，初始化参数

```python
net = nn.Sequential(nn.Flatten(),nn.Linear(784,256),nn.ReLU(),nn.Linear(256,10))

def init_weight(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)
    
net.apply(init_weight)
```

### 4.3.2 定义超参数，loss和updater并训练

```python
batch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss()
trainer = torch.optim.SGD(net.parameters(), lr=lr)

train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

## 4.4 模型选择、过拟合和欠拟合

### 4.4.1 训练误差和泛化误差

**训练误差**：模型在训练数据上的误差（不关心的）

**泛化误差**：模型在新数据上的误差（我们关心的）

### 4.4.2 模型选择

**训练数据集**：训练模型参数，用于比较模型和模型

**验证数据集**：选择模型超参数，用于比较一个模型，不同超参数选择的情况

- 如拿50%出来用作训练数据
- 跟训练数据不同；类似平常小测验，周考；
- 在验证集上评估模型

**测试数据集**：只用一次的数据集

- 类似高考（QAQ）

**K-则交叉验证**：

在没有足够多的数据时使用

- 将训练数据分割成k块
- for i = 1，...，K
  - 使用第i块作为验证集，其他作为训练集，执行K次模型训练和验证；每次用不同超参数的模型
- 报告K个验证集误差的平均

常用的K为5或10

### 4.4.3 过拟合和欠拟合

| 列：模型容量；行：数据 | 简单   | 复杂   |
| ---------------------- | ------ | ------ |
| 低                     | 正常   | 欠拟合 |
| 高                     | 过拟合 | 正常   |

**模型容量：**

- 拟合各种函数的能力
- 低容量的模型难以拟合训练数据
- 高容量的模型可以记住所有的训练数据

![image-20220102011721008](C:\Users\DAI FENGYUAN\AppData\Roaming\Typora\typora-user-images\image-20220102011721008.png)

模型容量评估（不同算法不好评估，只针对同一个模型种类）：

- 参数的个数
- 参数值的选择范围

## 4.5 权重衰退 weight decay

常见的处理过拟合的方法 ：限制参数值的选择范围
$$
\min{ l（w,b）}    subject to ||w||^2 \leq \theta
$$

- 通常不限制偏移b（限不限制都差不多）
- 更小的theta意味着更强的正则项

上式等价于：
$$
\min{l(w,b)}+\frac{\lambda}{2}*||w||^2
$$
超参数lambda控制了正则项的重要程度

- lambda=0  无作用
- lambda->无穷，w->0



实践内容其实差不多，就是在计算loss时加了一个L1_penalty，再计算梯度

## 4.6 丢弃法 Dropout

也是处理过拟合的方法（跟正则项效果是一样的）

在训练过程中，在计算后续层之前向网络的每一层注入噪声。

**如何注入噪声？**

- 我们希望加入噪声过后h的数学期望不变，对h中的每一个元素做如下扰动；其中p是超参数

![image-20220102162654080](C:\Users\DAI FENGYUAN\AppData\Roaming\Typora\typora-user-images\image-20220102162654080.png)

- 只在训练中进行：影响模型参数的更新；

- 输出时并不用dropout；

## 4.8 数值稳定性和模型初始化

nan：一个数除0；数值太大

inf  ：lr调太大了

建议lr往小里调，直到能正确地出一些值

### 4.8.1 梯度消失和梯度爆炸

对一个L层的MLP，若L太大，最后求导的结果可能走向极端：（这部分有关矩阵求导，原理其实看不大懂😞）

- 梯度爆炸
- 梯度消失

因此，这种模型要么对学习率很敏感，可能需要在训练过程中不断调整学习率；要么不管如何选择学习率，训练都没有进展

### 4.8.2 如何让训练更加稳定

**让乘法变加法**

- ResNet，LSTM

**归一化**

- 梯度归一化，梯度剪裁

**合理的权重初始和激活函数**   -> 为了提升数值稳定性

目标：MLP中每一层输出的方差和期望一致

因此初始化权重时，要求满足

前向传播
$$
n_{t-1}γ_t = 1
$$
反向传播
$$
n_{t}γ_t = 1
$$
其中n为第t层的输入数据个数，γ为第t层权重的方差

但是很难同时满足上述两个条件（除非输入个数等于输出个数）

**解决方法：**

Xavier使得上两式的均值为1，即 γ{t} = 2/(n{t-1}+n{t})

因此初始化权重时，要求

<img src="C:\Users\DAI FENGYUAN\AppData\Roaming\Typora\typora-user-images\image-20220103185707893.png" alt="image-20220103185707893" style="zoom: 50%;" />

或者

<img src="C:\Users\DAI FENGYUAN\AppData\Roaming\Typora\typora-user-images\image-20220103185748679.png" alt="image-20220103185748679" style="zoom:50%;" />

检查激活函数：

- tanh和relu都可以在一定程度上近似或者直接看作正比例函数
- sigmoid需要调整： 4*sigmoid - 2

## 4.9 实战Kaggle比赛

**Q：将类别变量转换成伪变量时内存爆了咋办？**

A：要么改用稀疏矩阵；要么不用one hot，用其他方法或者忽略0.0；

**Q：训练时loss图像抖动厉害？**

A：lr太大了或者是batch太小

# 5 深度学习计算

## 5.1 层和块

任何一个层或神经网络都是module的子类；

**自定义块MLP：**

```python
class MLP(nn.Module):
    # 用模型参数声明层。这里，我们声明两个全连接的层
    def __init__(self):
        # 调用MLP的父类Module的构造函数来执行必要的初始化。
        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）
        super().__init__()
        self.hidden = nn.Linear(20, 256)  # 隐藏层
        self.out = nn.Linear(256, 10)  # 输出层

    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出
    def forward(self, X):
        # 注意，这里我们使用ReLU的函数版本，其在nn.functional模块中定义。
        return self.out(F.relu(self.hidden(X)))
```

等价于下面代码：

```python
import torch
from torch import nn
from torch.nn import functional as F

net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))

X = torch.rand(2, 20)
net(X)
```

**定义顺序块：**

self_modules是一个ordered dictionary·

```python
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for idx, module in enumerate(args):
            # 这里，module是Module子类的一个实例。我们把它保存在'Module'类的成员
            # 变量_modules中。module的类型是OrderedDict
            self._modules[str(idx)] = module

    def forward(self, X):
        # OrderedDict保证了按照成员添加的顺序遍历它们
        for block in self._modules.values():
            X = block(X)
        return X
```

通过定义forward，可以自定义乱七八糟的块（可以做得很灵活）：

```python
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # 不计算梯度的随机权重参数。因此其在训练期间保持不变
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20)

    def forward(self, X):
        X = self.linear(X)
        # 使用创建的常量参数以及relu和mm函数
        X = F.relu(torch.mm(X, self.rand_weight) + 1)
        # 复用全连接层。这相当于两个全连接层共享参数
        X = self.linear(X)
        # 控制流
        while X.abs().sum() > 1:
            X /= 2
        return X.sum()
```

也可以套娃：

```python
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),
                                 nn.Linear(64, 32), nn.ReLU())
        self.linear = nn.Linear(32, 16)

    def forward(self, X):
        return self.linear(self.net(X))

chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())
chimera(X)
```

## 5.2 参数管理

### 5.2.1 访问参数

**通过下面代码访问第二层的模型参数：**

```python
print(net[2].state_dict())
```

结果：

```html
OrderedDict([('weight', tensor([[-0.2680, -0.3387,  0.0259, -0.0591,  0.1884,  0.2721,  0.1892,  0.2496]])), ('bias', tensor([0.3248]))])
```

**一次性访问所有参数：**

```python
print([(name, param.shape) for name, param in net[0].named_parameters()])
print([(name, param.shape) for name, param in net.named_parameters()])
```

结果：

```html
('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))
('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))
```

访问最后一层的偏移：

```python
net.state_dict()['2.bias'].data
```

结果：

```html
tensor([0.3248])
```

可以用print(net)来打印网络结构

### 5.2.2 内置初始化

apply：对模型所有的层使用某函数

对类似normal\_，zeros\_的函数,在函数后面加\_表示替换

**基本初始化**

```python
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init.zeros_(m.bias)
        
net.apply(init_normal)
```

**对某些块应用不同的初始化**

```python
def xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)
def init_42(m):
    if type(m) == nn.Linear:
        nn.init.constant_(m.weight, 42)

net[0].apply(xavier)
net[2].apply(init_42)
```

**自定义初始化**

目标：![image-20220104162944403](C:\Users\DAI FENGYUAN\AppData\Roaming\Typora\typora-user-images\image-20220104162944403.png)

```python
def my_init(m):
    if type(m) == nn.Linear:
        print("Init", *[(name, param.shape)
                        for name, param in m.named_parameters()][0])
        nn.init.uniform_(m.weight, -10, 10)
        m.weight.data *= m.weight.data.abs() >= 5

net.apply(my_init)
```

**暴力方法：**

```
net[0].weight.data[:] += 1
net[0].weight.data[0, 0] = 42
```

### 5.2.3 参数绑定

希望在多个层之间共享参数

```python
# 我们需要给共享层一个名称，以便可以引用它的参数
shared = nn.Linear(8, 8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                    shared, nn.ReLU(),
                    shared, nn.ReLU(),
                    nn.Linear(8, 1))
net(X)
# 检查参数是否相同
print(net[2].weight.data[0] == net[4].weight.data[0])
net[2].weight.data[0, 0] = 100
# 确保它们实际上是同一个对象，而不只是有相同的值
print(net[2].weight.data[0] == net[4].weight.data[0])
```

输出全是True

## 5.3 自定义层

nn.module中，直接调用net(X)等价于net.forward(X)，所以定义forward就是定义运算

### 5.3.1 不带参数的层

一个展示每个元素的离均值的便宜程度的层

```python
import torch
import torch.nn.functional as F
from torch import nn


class CenteredLayer(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, X):
        return X - X.mean()
```

```python
layer = CenteredLayer()
layer(torch.FloatTensor([1, 2, 3, 4, 5]))
```

```html
tensor([-2., -1.,  0.,  1.,  2.])
```

### 5.3.2 带参数的层

其实就是线性层

```python
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))
    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return F.relu(linear)
```

print一下：

```python
linear = MyLinear(5, 3)
linear.weight
```

```html
Parameter containing:
tensor([[ 2.2578, -0.4571, -1.4185],
        [ 1.6469, -0.4779,  1.0448],
        [ 0.9764,  0.4048,  0.6039],
        [ 0.9259,  1.2831, -0.4907],
        [ 1.1768, -0.2245,  1.2883]], requires_grad=True)
```

使用自定义层（其实跟torch里面的linear一样）

```python
linear(torch.rand(2, 5))
```

结果：

```html
tensor([[0.9983, 1.5834, 0.1999],
        [2.7293, 0.3060, 1.1866]])
```

可以放Sequential里面

```python
net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))
net(torch.rand(2, 64))
```

结果

```html
tensor([[0.],
        [0.]])
```

## 5.4 读写文件

### 5.4.1 加载和保存张量

调用load和save分别读写

保存的元素x可以是张量，列表以及字典

```python
import torch
from torch import nn
from torch.nn import functional as F

x = torch.arange(4)
torch.save(x, 'x-file')
```

```python
x2 = torch.load('x-file')
```

### 5.4.2 加载和保存模型参数

只能存模型的权重和偏移

```python
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.output = nn.Linear(256, 10)

    def forward(self, x):
        return self.output(F.relu(self.hidden(x)))

net = MLP()
X = torch.randn(size=(2, 20))
Y = net(X)

torch.save(net.state_dict(), 'mlp.params')
```

load前需要知道原先的模型结构

```python
clone = MLP()
clone.load_state_dict(torch.load('mlp.params'))
clone.eval()  # Sets the module in evaluation mode. 在这里只是想print一下
```

## 5.5 使用GPU

### 5.5.1 设备

查看设备信息：

```python
!nvidia-smi
```

查询可用gpu数量

```python
torch.cuda.device_count()
```

两个函数，方便我们在存在GPU和不存在的时候都可以使用

```python
def try_gpu(i=0):  #@save
    """如果存在，则返回gpu(i)，否则返回cpu()"""
    if torch.cuda.device_count() >= i + 1:
        return torch.device(f'cuda:{i}')
    return torch.device('cpu')

def try_all_gpus():  #@save
    """返回所有可用的GPU，如果没有GPU，则返回[cpu(),]"""
    devices = [torch.device(f'cuda:{i}')
             for i in range(torch.cuda.device_count())]
    return devices if devices else [torch.device('cpu')]

```



### 5.5.2 张量和GPU

查询张量所在设备，默认在cpu上，注意无论何时我们要对多个项进行操作， 它们都必须在同一个设备上；

```python
x = torch.tensor([1, 2, 3])
x.device
```

返回device(type='cpu')；

将张量存储在GPU上

```python
X = torch.ones(2, 3, device=try_gpu())
X
```

返回

```python
tensor([[1., 1., 1.],
        [1., 1., 1.]], device='cuda:0')
```

将cpu上的tensor变量x在gpu中复制一份

```python
Z = X.cuda(0)  # Z = X.cuda()
X + Z 
```

### 5.5.3 神经网络和GPU

将模型放到GPU上

```python
net = nn.Sequential(nn.Linear(3, 1))
net = net.to(device=try_gpu())
```

